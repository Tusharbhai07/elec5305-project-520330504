# -*- coding: utf-8 -*-
"""ultrasound_8k_Baseline_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UmBXJsQaG0xNVGXPCGg-ejU4b0tel4ij
"""

# ELEC5305: Final Project (UrbanSound8K Automobile Sounds)
# Author: Tushar Manish Khupte (SID: 520330504)
# Purpose: Reproducible end-to-end pipeline with example inputs matched to example outputs
#          so readers can see and hear what the model predicts.

# Quick start (Colab/local):
#   python elec5305_hd_final_project.py
#   python elec5305_hd_final_project.py --audio 2   # export audio pairs (2 per class)
#   python elec5305_hd_final_project.py --plot      # regenerate plots from saved CSV/JSON

# Requirements:
#   numpy==2.2.2, librosa==0.10.2.post1, soundfile==0.12.1, matplotlib==3.9.2,
#   scikit-learn==1.6.1, tqdm==4.67.1, pywt==1.6.0, torch (Colab default OK), pandas

import os, sys, json, glob, shutil, tarfile, zipfile, subprocess, hashlib, random
from pathlib import Path
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd
import librosa, librosa.display
import matplotlib.pyplot as plt
import soundfile as sf
from tqdm import tqdm

import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# --------------------------
# 1) Configuration
# --------------------------
CFG = {
    "root": "/content/UrbanSound8K",
    "metadata_csv": "metadata/UrbanSound8K.csv",
    "folds_dir": "audio",

    # classes (auto + confusers)
    "classes": ["car_horn","engine_idling","siren","dog_bark","drilling","street_music"],

    # audio/features
    "sr": 22050,
    "clip_seconds": 4.0,
    "n_mels": 64,
    "n_fft": 1024,
    "hop_length": 256,

    # training
    "batch_size": 64,
    "epochs": 30,
    "lr": 2e-3,
    "weight_decay": 1e-4,
    "seed": 1337,
    "num_workers": 2,

    # speed control
    "fast_mode": True,
    "fast_cap_per_class": 400,

    # research toggles
    "use_wiener": False,
    "use_modspec": True,
    "use_lpc": True,
    "specaugment": True,
    "model": "cnn_bilstm",  # "tinycnn" or "cnn_bilstm"

    # cache/outputs
    "cache_dir": "/content/us8k_cache",
    "out_dir": "/content/outputs",
}

# Determinism
random.seed(CFG["seed"]); np.random.seed(CFG["seed"]); torch.manual_seed(CFG["seed"])
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
os.makedirs(CFG["cache_dir"], exist_ok=True)
os.makedirs(CFG["out_dir"], exist_ok=True)

# --------------------------
# 2) Data: Download / Prepare
# --------------------------
root = Path(CFG["root"])
meta_path = root / CFG["metadata_csv"]

def _already_present() -> bool:
    return meta_path.exists()

def _try_wget(url: str, out: Path) -> bool:
    try:
        print(f"Attempting download: {url}")
        ret = subprocess.run(["wget", "-q", "--show-progress", "-O", str(out), url])
        return ret.returncode == 0 and out.exists() and out.stat().st_size > 10_000_000
    except Exception as e:
        print("wget error:", e)
        return False

def _extract_archive(archive_path: Path, dest: Path):
    if archive_path.suffixes[-2:] == ['.tar', '.gz'] or archive_path.suffix == ".tgz":
        with tarfile.open(archive_path, 'r:gz') as tar:
            tar.extractall(dest)
    elif archive_path.suffix == ".zip":
        with zipfile.ZipFile(archive_path, 'r') as z:
            z.extractall(dest)
    else:
        raise ValueError(f"Unknown archive type: {archive_path}")

def ensure_dataset():
    if _already_present():
        print("✅ Found UrbanSound8K; skipping download.")
        return
    print("UrbanSound8K not found. Will attempt to download using known mirrors.")
    dl_dir = Path("/content/_dl"); dl_dir.mkdir(exist_ok=True, parents=True)
    archive = dl_dir / "UrbanSound8K.tar.gz"
    mirrors = [
        "https://zenodo.org/records/1203745/files/UrbanSound8K.tar.gz?download=1",
        "https://urbansounddataset.weebly.com/uploads/5/3/6/2/5362550/urbansound8k.tar.gz",
    ]
    ok = False
    for url in mirrors:
        if _try_wget(url, archive):
            ok = True
            break
    if not ok:
        print("\n❗ Mirror download failed. Please download officially and place under /content")
        print("   Official page: https://urbansounddataset.weebly.com/urbansound8k.html")
        raise SystemExit
    print("Extracting...")
    _extract_archive(archive, Path("/content"))
    # Normalize structure if nested differently
    if not (root / "metadata/UrbanSound8K.csv").exists():
        candidates = list(Path("/content").glob("**/UrbanSound8K/metadata/UrbanSound8K.csv"))
        if len(candidates):
            found_root = candidates[0].parents[1]
            if str(found_root) != str(root):
                shutil.move(str(found_root), str(root))
    print("✅ Dataset ready.")

# --------------------------
# 3) Dataset & Features
# --------------------------
class US8KSubset(Dataset):
    def __init__(self, df: pd.DataFrame, cfg: Dict):
        self.df = df.reset_index(drop=True)
        self.cfg = cfg
        self.sr = cfg["sr"]
        self.clip_len = int(cfg["clip_seconds"] * self.sr)
        self.melspec_cache = Path(cfg["cache_dir"]) / "mels"
        self.melspec_cache.mkdir(parents=True, exist_ok=True)

    def __len__(self):
        return len(self.df)

    def _cache_key(self, wav_path: str) -> Path:
        h = hashlib.sha1(str(wav_path).encode()).hexdigest()[:16]
        return self.melspec_cache / f"{h}.npy"

    def _apply_wiener(self, y: np.ndarray) -> np.ndarray:
        S = librosa.stft(y, n_fft=self.cfg["n_fft"], hop_length=self.cfg["hop_length"], window="hann")
        mag, phase = np.abs(S), np.angle(S)
        noise = np.percentile(mag, 10, axis=1, keepdims=True)
        gain = (mag**2) / (mag**2 + noise**2 + 1e-9)
        Sout = gain * mag * np.exp(1j * phase)
        y_hat = librosa.istft(Sout, hop_length=self.cfg["hop_length"], window="hann", length=len(y))
        return y_hat

    def _load_clip(self, path: str) -> np.ndarray:
        y, sr = librosa.load(path, sr=self.sr, mono=True)
        if len(y) < self.clip_len:
            y = np.pad(y, (0, self.clip_len - len(y)))
        else:
            y = y[:self.clip_len]
        return y

    def _wav_to_mel(self, y: np.ndarray) -> np.ndarray:
        if self.cfg.get("use_wiener", False):
            y = self._apply_wiener(y)
        S = librosa.feature.melspectrogram(
            y=y, sr=self.sr, n_fft=self.cfg["n_fft"], hop_length=self.cfg["hop_length"],
            n_mels=self.cfg["n_mels"], power=2.0
        )
        S_db = librosa.power_to_db(S, ref=np.max)
        mu, sigma = S_db.mean(), S_db.std() + 1e-6
        S_norm = (S_db - mu) / sigma
        return S_norm.astype(np.float32)

    def __getitem__(self, idx: int):
        r = self.df.iloc[idx]
        path = (os.path.join(self.cfg["root"], self.cfg["folds_dir"], f"fold{r.fold}", r.slice_file_name)
                if "path" not in r else r["path"])
        label = int(r["target"])

        # mel (cached)
        ck = self._cache_key(path)
        if os.path.exists(ck):
            mel = np.load(ck)
        else:
            y = self._load_clip(path)
            mel = self._wav_to_mel(y)
            np.save(ck, mel)

        # aux (ZCR/RMS/ModSpec/LPC)
        y0 = self._load_clip(path)
        zcr = librosa.feature.zero_crossing_rate(y0, frame_length=self.cfg["n_fft"], hop_length=self.cfg["hop_length"]).mean()
        rms = librosa.feature.rms(y=y0, frame_length=self.cfg["n_fft"], hop_length=self.cfg["hop_length"]).mean()
        feats = [float(zcr), float(rms)]

        if self.cfg.get("use_modspec", True):
            Sm = librosa.feature.melspectrogram(y=y0, sr=self.sr, n_fft=512, hop_length=128, n_mels=32, power=2.0)
            Sm = librosa.power_to_db(Sm, ref=np.max); Sm = (Sm - Sm.mean())/(Sm.std()+1e-6)
            ph, pw = 8, 8
            acc = []
            H, W = Sm.shape
            for i in range(0, H - ph + 1, ph):
                for j in range(0, W - pw + 1, pw):
                    P = Sm[i:i + ph, j:j + pw]
                    F = np.abs(np.fft.fft2(P))
                    acc.append(F)
            Fmean = np.mean(acc, axis=0) if acc else np.zeros((ph, pw))
            mod_vec = np.sort(Fmean.ravel())[-20:].tolist()
            feats += [float(x) for x in mod_vec]

        if self.cfg.get("use_lpc", True):
            try:
                lpc = librosa.lpc(y0, order=10).tolist()
            except Exception:
                lpc = [0.0] * 11
            feats += [float(x) for x in lpc]

        mel = torch.from_numpy(mel)[None, ...]
        aux = torch.tensor([feats], dtype=torch.float32)
        return (mel, aux), torch.tensor(label, dtype=torch.long)

# --------------------------
# 4) Models
# --------------------------
class TinyCNNWithAux(nn.Module):
    def __init__(self, n_mels: int, n_classes: int, aux_dim: int):
        super().__init__()
        self.feat = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32,64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
        )
        self.gap = nn.AdaptiveAvgPool2d((1,1))
        self.head = nn.Sequential(
            nn.Linear(64 + aux_dim, 192), nn.ReLU(), nn.Dropout(0.25),
            nn.Linear(192, n_classes)
        )
    def forward(self, mel, aux):
        x = self.feat(mel)
        x = self.gap(x).squeeze(-1).squeeze(-1)
        x = torch.cat([x, aux.squeeze(1)], dim=1)
        return self.head(x)

class CNNBiLSTM(nn.Module):
    def __init__(self, n_mels: int, n_classes: int, aux_dim: int):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32,64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),
        )
        self.proj = nn.Linear((n_mels//4)*64, 128)
        self.rnn  = nn.LSTM(128, 96, batch_first=True, bidirectional=True)
        self.fc   = nn.Sequential(
            nn.Linear(96*2 + aux_dim, 192), nn.ReLU(), nn.Dropout(0.25),
            nn.Linear(192, n_classes)
        )
    def forward(self, mel, aux):
        x = self.cnn(mel)              # B, C, H, W
        B,C,H,W = x.shape
        x = x.permute(0,3,1,2).contiguous().view(B, W, C*H)  # B, T, F
        x,_ = self.rnn(self.proj(x))   # B, T, 2*96
        x = x.mean(1)                  # temporal mean
        x = torch.cat([x, aux.squeeze(1)], dim=1)
        return self.fc(x)

# --------------------------
# 5) Utilities: SpecAug, EarlyStopping, Plots
# --------------------------
def spec_augment(mel, time_masks=1, freq_masks=1, max_t=24, max_f=8):
    if not CFG.get("specaugment", True):
        return mel
    B,_,F,T = mel.shape
    m = mel.clone()
    for _ in range(freq_masks):
        f = np.random.randint(0, max_f+1)
        f0 = np.random.randint(0, max(1, F-f))
        m[:,:,f0:f0+f,:] = 0
    for _ in range(time_masks):
        t = np.random.randint(0, max_t+1)
        t0 = np.random.randint(0, max(1, T-t))
        m[:,:,:,t0:t0+t] = 0
    return m

class EarlyStopper:
    def __init__(self, patience=6, mode='max'):
        self.patience = patience
        self.mode = mode
        self.best = None
        self.count = 0
    def step(self, value):
        if self.best is None:
            self.best = value; self.count = 0; return False
        improve = (value > self.best) if self.mode=='max' else (value < self.best)
        if improve:
            self.best = value; self.count = 0; return False
        self.count += 1
        return self.count > self.patience

def save_confusion(cm: np.ndarray, labels: List[str], out_png: Path, out_csv: Path):
    fig, ax = plt.subplots(figsize=(5,5))
    im = ax.imshow(cm, interpolation='nearest')
    ax.set_title("Confusion Matrix"); ax.set_xlabel("Predicted"); ax.set_ylabel("True")
    ax.set_xticks(range(len(labels))); ax.set_yticks(range(len(labels)))
    ax.set_xticklabels(labels, rotation=45, ha='right'); ax.set_yticklabels(labels)
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, cm[i, j], ha="center", va="center")
    plt.tight_layout(); fig.savefig(out_png, dpi=160); plt.close(fig)
    pd.DataFrame(cm, index=labels, columns=labels).to_csv(out_csv)

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

# --------------------------
# 6) Training / Evaluation
# --------------------------
def build_splits(meta: pd.DataFrame, label2id: Dict[str,int]):
    FOLDS = list(range(1, 11))
    test_fold, val_fold = 10, 9
    train_folds = [f for f in FOLDS if f not in (test_fold, val_fold)]

    keep = meta[meta["class"].isin(CFG["classes"])].copy()
    keep["target"] = keep["class"].map(label2id)

    train_df = keep[keep["fold"].isin(train_folds)].reset_index(drop=True)
    val_df   = keep[keep["fold"] == val_fold].reset_index(drop=True)
    test_df  = keep[keep["fold"] == test_fold].reset_index(drop=True)

    def cap_per_class(df, cap):
        if not cap: return df
        out = []
        for c in CFG["classes"]:
            dfc = df[df["class"] == c]
            n = min(cap, len(dfc))
            out.append(dfc.sample(n=n, random_state=CFG["seed"]) if n > 0 else dfc)
        return pd.concat(out).sample(frac=1.0, random_state=CFG["seed"]).reset_index(drop=True)

    if CFG["fast_mode"]:
        train_df = cap_per_class(train_df, CFG["fast_cap_per_class"])
        val_df   = cap_per_class(val_df, min(200, CFG["fast_cap_per_class"]))

    return train_df, val_df, test_df

def infer_aux_dim(train_ds):
    tmp = DataLoader(train_ds, batch_size=1, shuffle=False)
    (mel1, aux1), _ = next(iter(tmp))
    return aux1.shape[-1]

def run_epoch(model, loader, device, optimizer=None):
    is_train = optimizer is not None
    model.train(is_train)
    total, correct, loss_sum = 0, 0, 0.0
    y_true, y_pred = [], []
    crit = nn.CrossEntropyLoss()
    for (mel, aux), yb in loader:
        mel = mel.to(device, non_blocking=True).float()
        aux = aux.to(device, non_blocking=True).float()
        yb  = yb.to(device, non_blocking=True).long()
        if is_train:
            optimizer.zero_grad()
            mel = spec_augment(mel)
        logits = model(mel, aux)
        loss = crit(logits, yb)
        if is_train:
            loss.backward(); optimizer.step()
        pred = logits.argmax(1)
        total += yb.size(0)
        correct += (pred == yb).sum().item()
        loss_sum += loss.item() * yb.size(0)
        y_true.extend(yb.detach().cpu().tolist())
        y_pred.extend(pred.detach().cpu().tolist())
    acc = correct/total
    macro_f1 = f1_score(y_true, y_pred, average='macro')
    return loss_sum/total, acc, macro_f1

def train_and_eval():
    ensure_dataset()

    # metadata
    meta = pd.read_csv(meta_path)
    meta["fname"] = meta["slice_file_name"]
    meta["fold_path"] = meta["fold"].apply(lambda f: str(Path(CFG["root"]) / CFG["folds_dir"] / f"fold{f}"))
    meta["path"] = meta.apply(lambda r: str(Path(r["fold_path"]) / r["fname"]), axis=1)

    label2id = {c: i for i, c in enumerate(sorted(CFG["classes"]))}
    id2label = {v:k for k,v in label2id.items()}

    train_df, val_df, test_df = build_splits(meta, label2id)

    train_ds = US8KSubset(train_df, CFG)
    val_ds   = US8KSubset(val_df, CFG)
    test_ds  = US8KSubset(test_df, CFG)

    train_dl = DataLoader(train_ds, batch_size=CFG["batch_size"], shuffle=True,  num_workers=CFG["num_workers"], pin_memory=True)
    val_dl   = DataLoader(val_ds,   batch_size=CFG["batch_size"], shuffle=False, num_workers=CFG["num_workers"], pin_memory=True)
    test_dl  = DataLoader(test_ds,  batch_size=CFG["batch_size"], shuffle=False, num_workers=CFG["num_workers"], pin_memory=True)

    aux_dim = infer_aux_dim(train_ds)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if CFG.get("model","cnn_bilstm") == "cnn_bilstm":
        model = CNNBiLSTM(CFG["n_mels"], n_classes=len(label2id), aux_dim=aux_dim).to(device)
    else:
        model = TinyCNNWithAux(CFG["n_mels"], n_classes=len(label2id), aux_dim=aux_dim).to(device)

    opt = torch.optim.AdamW(model.parameters(), lr=CFG["lr"], weight_decay=CFG["weight_decay"])
    stopper = EarlyStopper(patience=6, mode='max')
    best_val_f1 = -1.0

    hist_rows = []
    for ep in range(1, CFG["epochs"]+1):
        tr_loss, tr_acc, tr_f1 = run_epoch(model, train_dl, device, optimizer=opt)
        va_loss, va_acc, va_f1 = run_epoch(model, val_dl, device, optimizer=None)
        hist_rows.append({"epoch": ep, "train_loss": tr_loss, "train_acc": tr_acc, "train_f1": tr_f1,
                          "val_loss": va_loss, "val_acc": va_acc, "val_f1": va_f1})
        print(f"Epoch {ep:02d} | train: loss={tr_loss:.4f} acc={tr_acc:.3f} f1={tr_f1:.3f}  "
              f"| val: loss={va_loss:.4f} acc={va_acc:.3f} f1={va_f1:.3f}")
        if va_f1 > best_val_f1:
            best_val_f1 = va_f1
            torch.save(model.state_dict(), f"{CFG['out_dir']}/best.pt")
        if stopper.step(va_f1):
            print("Early stopping triggered.")
            break

    pd.DataFrame(hist_rows).to_csv(f"{CFG['out_dir']}/training_log.csv", index=False)

    # test with best
    model.load_state_dict(torch.load(f"{CFG['out_dir']}/best.pt", map_location=device))
    model.eval()

    all_y, all_p = [], []
    with torch.no_grad():
        for (mel, aux), yb in test_dl:
            mel = mel.to(device).float()
            aux = aux.to(device).float()
            logits = model(mel, aux)
            preds = logits.argmax(1).cpu().numpy().tolist()
            all_p += preds
            all_y += yb.numpy().tolist()

    labels_in_order = [k for k,_ in sorted(label2id.items(), key=lambda x:x[1])]
    report = classification_report(all_y, all_p, target_names=labels_in_order, output_dict=True)
    macro_f1_test = f1_score(all_y, all_p, average='macro')
    cm = confusion_matrix(all_y, all_p)

    with open(f"{CFG['out_dir']}/metrics.json", "w") as f:
        json.dump({"test_macro_f1": macro_f1_test, "test_report": report}, f, indent=2)

    save_confusion(cm, labels_in_order,
                   out_png=Path(CFG['out_dir'])/"confusion_matrix.png",
                   out_csv=Path(CFG['out_dir'])/"confusion_matrix.csv")

    torch.save(model.state_dict(), f"{CFG['out_dir']}/final.pt")
    print("Artifacts saved to:", CFG["out_dir"])

    # Showcase: inputs matched with outputs
    showcase_examples(model, device, val_df, label2id, split_name="val",  k=2)
    showcase_examples(model, device, test_df, label2id, split_name="test", k=2)
    showcase_mistakes(model, device, test_df, label2id, k=4)

    return model, device, label2id

# --------------------------
# 7) Showcase: Examples & Mistakes
# --------------------------
def _plot_and_save_mel(y: np.ndarray, sr: int, out_png: Path, title: str="Log-mel"):
    ensure_dir(out_png.parent)
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=CFG["n_fft"], hop_length=CFG["hop_length"], n_mels=CFG["n_mels"], power=2.0)
    S_db = librosa.power_to_db(S, ref=np.max)
    fig, ax = plt.subplots(figsize=(5.0, 3.0))
    img = librosa.display.specshow(S_db, sr=sr, hop_length=CFG["hop_length"], x_axis='time', y_axis='mel', ax=ax)
    ax.set_title(title)
    fig.colorbar(img, ax=ax, format='%+2.0f dB')
    plt.tight_layout(); fig.savefig(out_png, dpi=160); plt.close(fig)

def _predict_raw(model, device, wav_path: str, id2label: Dict[int,str]) -> Tuple[str, Dict[str, float]]:
    ds = US8KSubset(pd.DataFrame([{ "path": wav_path, "target": 0 }]), CFG)
    (mel, aux), _ = ds[0]
    with torch.no_grad():
        logits = model(
            mel.unsqueeze(0).to(device).float(),
            aux.unsqueeze(0).to(device).float()
        )
        p = logits.softmax(1).cpu().numpy()[0]
    pred = int(np.argmax(p))
    return id2label[pred], {id2label[i]: float(p[i]) for i in range(len(p))}

def showcase_examples(model, device, df: pd.DataFrame, label2id: Dict[str,int], split_name="val", k=2):
    id2label = {v:k for k,v in label2id.items()}
    out_dir = Path(CFG['out_dir'])/"examples"/split_name
    ensure_dir(out_dir)
    rows = []
    for cls in sorted(label2id.keys()):
        sdf = df[df["class"]==cls]
        if len(sdf)==0: continue
        picks = sdf.sample(n=min(k, len(sdf)), random_state=CFG["seed"]).reset_index(drop=True)
        for _, r in picks.iterrows():
            wav = r["path"]
            y, sr = librosa.load(wav, sr=CFG["sr"])
            pred, probs = _predict_raw(model, device, wav, id2label)
            spec_path = out_dir/f"{Path(wav).stem}__{cls}__pred-{pred}.png"
            _plot_and_save_mel(y, sr, spec_path, title=f"{split_name} • {cls} → {pred}")
            rows.append({
                "split": split_name,
                "file": os.path.basename(wav),
                "path": wav,
                "true": cls,
                "pred": pred,
                "top1_prob": float(max(probs.values())),
                "probs_json": json.dumps(probs),
                "spectrogram_png": str(spec_path)
            })
    df_out = pd.DataFrame(rows)
    df_out.to_csv(out_dir/"examples_inputs_outputs.csv", index=False)
    print(f"Saved examples for {split_name} →", out_dir)

def showcase_mistakes(model, device, df: pd.DataFrame, label2id: Dict[str,int], k=4):
    id2label = {v:k for k,v in label2id.items()}
    out_dir = Path(CFG['out_dir'])/"examples"/"test_mistakes"
    ensure_dir(out_dir)
    ds = US8KSubset(df, CFG)
    dl = DataLoader(ds, batch_size=CFG["batch_size"], shuffle=False, num_workers=CFG["num_workers"])
    mistakes, base = [], 0
    with torch.no_grad():
        for (mel, aux), yb in dl:
            mel, aux = mel.to(device).float(), aux.to(device).float()
            logits = model(mel, aux)
            preds = logits.argmax(1).cpu()
            yb = yb.cpu()
            for j in range(len(yb)):
                if preds[j].item() != yb[j].item():
                    row = df.iloc[base + j]
                    wav = row["path"]
                    true_c = id2label[int(yb[j].item())]
                    pred_c = id2label[int(preds[j].item())]
                    y, sr = librosa.load(wav, sr=CFG["sr"])
                    spec_path = out_dir/f"{Path(wav).stem}__true-{true_c}__pred-{pred_c}.png"
                    _plot_and_save_mel(y, sr, spec_path, title=f"mistake • true: {true_c} → pred: {pred_c}")
                    mistakes.append({
                        "file": os.path.basename(wav),
                        "path": wav,
                        "true": true_c,
                        "pred": pred_c,
                        "spectrogram_png": str(spec_path)
                    })
                    if len(mistakes) >= k:
                        break
            base += len(yb)
            if len(mistakes) >= k:
                break
    pd.DataFrame(mistakes).to_csv(out_dir/"mistakes_examples.csv", index=False)
    print(f"Saved first {len(mistakes)} mistakes →", out_dir)

# --------------------------
# 8) Inference helpers
# --------------------------
def predict_file(wav_path: str, model=None, device=None, label2id: Dict[str,int]=None):
    if model is None or device is None or label2id is None:
        model, device, label2id = _lazy_load_best()
    id2label = {v:k for k,v in label2id.items()}
    pred, probs = _predict_raw(model, device, wav_path, id2label)
    print("Prediction:", pred)
    print("Probs:", json.dumps(probs, indent=2))
    return pred, probs

def batch_infer(folder: str, out_csv: str = None):
    model, device, label2id = _lazy_load_best()
    id2label = {v:k for k,v in label2id.items()}
    rows = []
    for wav in glob.glob(os.path.join(folder, "*.wav")):
        pred, probs = _predict_raw(model, device, wav, id2label)
        rows.append({"file": os.path.basename(wav), "pred": pred, "probs_json": json.dumps(probs)})
    df = pd.DataFrame(rows)
    if out_csv is None:
        out_csv = str(Path(CFG['out_dir'])/"custom_batch_predictions.csv")
    df.to_csv(out_csv, index=False)
    print("Saved:", out_csv)
    return df

def _lazy_load_best():
    ensure_dataset()
    meta = pd.read_csv(meta_path)
    label2id = {c: i for i, c in enumerate(sorted(CFG["classes"]))}
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # infer aux size quickly
    meta["fname"] = meta["slice_file_name"]
    meta["fold_path"] = meta["fold"].apply(lambda f: str(Path(CFG["root"]) / CFG["folds_dir"] / f"fold{f}"))
    meta["path"] = meta.apply(lambda r: str(Path(r["fold_path"]) / r["fname"]), axis=1)
    keep = meta[meta["class"].isin(CFG["classes"])].copy()
    keep["target"] = keep["class"].map(label2id)
    ds = US8KSubset(keep.iloc[[0]], CFG)
    (mel, aux), _ = ds[0]
    aux_dim = aux.shape[-1]

    if CFG.get("model","cnn_bilstm") == "cnn_bilstm":
        model = CNNBiLSTM(CFG["n_mels"], n_classes=len(label2id), aux_dim=aux_dim).to(device)
    else:
        model = TinyCNNWithAux(CFG["n_mels"], n_classes=len(label2id), aux_dim=aux_dim).to(device)

    ckpt = f"{CFG['out_dir']}/best.pt"
    assert os.path.exists(ckpt), f"Checkpoint not found: {ckpt}. Run training first."
    model.load_state_dict(torch.load(ckpt, map_location=device))
    model.eval()
    return model, device, label2id

# --------------------------
# 9) Audio exports (input vs processed output)
# --------------------------
def _wiener_like(y: np.ndarray) -> np.ndarray:
    S = librosa.stft(y, n_fft=CFG["n_fft"], hop_length=CFG["hop_length"], window="hann")
    mag, phase = np.abs(S), np.angle(S)
    noise = np.percentile(mag, 10, axis=1, keepdims=True)
    gain = (mag**2) / (mag**2 + noise**2 + 1e-9)
    Sout = gain * mag * np.exp(1j * phase)
    y_hat = librosa.istft(Sout, hop_length=CFG["hop_length"], window="hann", length=len(y))
    return y_hat.astype(np.float32)

def _mel_reconstruct(y: np.ndarray, sr: int) -> np.ndarray:
    S = librosa.feature.melspectrogram(
        y=y, sr=sr, n_fft=CFG["n_fft"], hop_length=CFG["hop_length"], n_mels=CFG["n_mels"], power=2.0
    )
    y_rec = librosa.feature.inverse.mel_to_audio(
        M=S, sr=sr, n_fft=CFG["n_fft"], hop_length=CFG["hop_length"], power=2.0
    )
    return y_rec.astype(np.float32)

def _safe_write(path: Path, y: np.ndarray, sr: int):
    y = np.asarray(y, dtype=np.float32)
    peak = float(np.max(np.abs(y)) + 1e-9)
    if peak > 1.0:
        y = y / peak
    sf.write(path, y, sr)

def export_audio_pairs(per_class: int = 1):
    """
    Export input and processed outputs (denoised + mel reconstruction) for a few examples per class.
    Writes to outputs/examples/audio_pairs/:
      <stem>__input.wav, <stem>__denoised.wav, <stem>__feature_recon.wav
      index.csv, README.md  (with <audio> players)
    Requires showcase_examples(...) to have run (it runs after training).
    """
    base = Path(CFG['out_dir']) / "examples"
    val_csv  = base / "val"  / "examples_inputs_outputs.csv"
    test_csv = base / "test" / "examples_inputs_outputs.csv"
    out_dir = base / "audio_pairs"
    out_dir.mkdir(parents=True, exist_ok=True)

    def _pick_rows(csv_path: Path, split_name: str):
        if not csv_path.exists():
            return []
        df = pd.read_csv(csv_path)
        df['__split'] = split_name
        chosen = []
        for c in sorted(df['true'].unique()):
            chosen += df[df['true'] == c].head(per_class).to_dict('records')
        return chosen

    rows = _pick_rows(val_csv, "val") + _pick_rows(test_csv, "test")
    if not rows:
        print("No examples CSVs found. Run training first so showcase_examples() creates them.")
        return

    index_rows = []
    for r in rows:
        y, sr = librosa.load(r['path'], sr=CFG['sr'], mono=True)
        stem = f"{r['true']}__{Path(r['file']).stem}__pred-{r['pred']}"
        wav_input = out_dir / f"{stem}__input.wav"
        wav_deno  = out_dir / f"{stem}__denoised.wav"
        wav_feat  = out_dir / f"{stem}__feature_recon.wav"

        _safe_write(wav_input, y, sr)
        _safe_write(wav_deno, _wiener_like(y), sr)
        _safe_write(wav_feat, _mel_reconstruct(y, sr), sr)

        # make spectrogram link portable for README
        spec_abs = Path(r['spectrogram_png'])
        if not spec_abs.is_absolute():
            spec_abs = (base / r['__split'] / Path(r['spectrogram_png']).name).resolve()
        spec_rel = os.path.relpath(spec_abs, out_dir)

        index_rows.append({
            "file": r['file'],
            "true": r['true'],
            "pred": r['pred'],
            "top1_prob": float(r['top1_prob']),
            "input_wav": wav_input.name,
            "denoised_wav": wav_deno.name,
            "feature_recon_wav": wav_feat.name,
            "spectrogram_png": spec_rel,
            "split": r['__split'],
        })

    pd.DataFrame(index_rows).to_csv(out_dir / "index.csv", index=False)

    lines = [
        "# Audio Pairs: Input vs Processed Output\n\n",
        "_Sample excerpts from UrbanSound8K for academic demonstration; see dataset metadata for original authors & licenses._<br>\n\n",
        "Each example provides the raw input, a Wiener-like denoised signal, and a mel-feature reconstruction (approx. what the model hears).\n\n",
    ]
    for it in index_rows:
        lines += [
            f"## {it['true']} → **{it['pred']}** (p≈{it['top1_prob']:.2f})\n\n",
            f"**Input (raw)**<br>\n<audio controls src='./{it['input_wav']}'></audio>\n\n",
            f"**Output 1 — Denoised**<br>\n<audio controls src='./{it['denoised_wav']}'></audio>\n\n",
            f"**Output 2 — Feature reconstruction**<br>\n<audio controls src='./{it['feature_recon_wav']}'></audio>\n\n",
            f"Spectrogram used by the model:<br>\n<img src='{it['spectrogram_png']}' width='360'/>\n\n---\n",
        ]
    (out_dir / "README.md").write_text("".join(lines), encoding="utf-8")
    print("Exported audio pairs to:", out_dir)

# --------------------------
# 10) Plotting utilities (post-training)
# --------------------------
def plot_training_curves(log_csv: str = None, out_dir: str = None):
    if log_csv is None:
        log_csv = str(Path(CFG['out_dir'])/"training_log.csv")
    if out_dir is None:
        out_dir = CFG['out_dir']
    df = pd.read_csv(log_csv)

    fig1, ax1 = plt.subplots(figsize=(6,4))
    ax1.plot(df['epoch'], df['train_loss'], label='train_loss')
    ax1.plot(df['epoch'], df['val_loss'], label='val_loss')
    ax1.set_xlabel('epoch'); ax1.set_ylabel('loss'); ax1.set_title('Loss vs Epochs'); ax1.legend(); plt.tight_layout()
    fig1.savefig(Path(out_dir)/'plot_loss.png', dpi=160); plt.close(fig1)

    fig2, ax2 = plt.subplots(figsize=(6,4))
    ax2.plot(df['epoch'], df['train_acc'], label='train_acc')
    ax2.plot(df['epoch'], df['val_acc'], label='val_acc')
    ax2.set_xlabel('epoch'); ax2.set_ylabel('accuracy'); ax2.set_title('Accuracy vs Epochs'); ax2.legend(); plt.tight_layout()
    fig2.savefig(Path(out_dir)/'plot_accuracy.png', dpi=160); plt.close(fig2)

    fig3, ax3 = plt.subplots(figsize=(6,4))
    ax3.plot(df['epoch'], df['train_f1'], label='train_macroF1')
    ax3.plot(df['epoch'], df['val_f1'], label='val_macroF1')
    ax3.set_xlabel('epoch'); ax3.set_ylabel('macro-F1'); ax3.set_title('Macro-F1 vs Epochs'); ax3.legend(); plt.tight_layout()
    fig3.savefig(Path(out_dir)/'plot_macro_f1.png', dpi=160); plt.close(fig3)
    print('Saved plots to:', out_dir)

def plot_per_class_f1(metrics_json: str = None, out_dir: str = None):
    if metrics_json is None:
        metrics_json = str(Path(CFG['out_dir'])/"metrics.json")
    if out_dir is None:
        out_dir = CFG['out_dir']
    with open(metrics_json, 'r') as f:
        metrics = json.load(f)
    rep = metrics.get('test_report', {})
    classes, f1s = [], []
    for cls, stats in rep.items():
        if isinstance(stats, dict) and 'f1-score' in stats and cls not in ('accuracy','macro avg','weighted avg'):
            classes.append(cls); f1s.append(stats['f1-score'])
    fig, ax = plt.subplots(figsize=(7,4))
    ax.bar(range(len(classes)), f1s)
    ax.set_xticks(range(len(classes))); ax.set_xticklabels(classes, rotation=30, ha='right')
    ax.set_ylim(0,1); ax.set_ylabel('F1-score'); ax.set_title('Per-class F1 (test)')
    plt.tight_layout(); fig.savefig(Path(out_dir)/'plot_per_class_f1.png', dpi=160); plt.close(fig)
    print('Saved per-class F1 plot to:', out_dir)

def regenerate_confusion_plot(conf_csv: str = None, out_dir: str = None):
    if conf_csv is None:
        conf_csv = str(Path(CFG['out_dir'])/"confusion_matrix.csv")
    if out_dir is None:
        out_dir = CFG['out_dir']
    df = pd.read_csv(conf_csv, index_col=0)
    cm = df.values.astype(int)
    labels = list(df.index)
    save_confusion(cm, labels, out_png=Path(out_dir)/'confusion_matrix.png', out_csv=Path(out_dir)/'confusion_matrix.csv')

# --------------------------
# 11) Main
# --------------------------
if __name__ == "__main__":
    if "--plot" in sys.argv:
        plot_training_curves()
        plot_per_class_f1()
        regenerate_confusion_plot()
    elif "--audio" in sys.argv or "--audio_pairs" in sys.argv:
        k = 1
        try:
            idx = sys.argv.index("--audio") if "--audio" in sys.argv else sys.argv.index("--audio_pairs")
            if idx + 1 < len(sys.argv):
                k = int(sys.argv[idx + 1])
        except Exception:
            pass
        export_audio_pairs(per_class=k)
    else:
        train_and_eval()

export_audio_pairs(per_class=2)  # change 2 to how many examples per class you want

!zip -r /content/outputs.zip /content/outputs
from google.colab import files
files.download("/content/outputs.zip")

import pandas as pd, librosa, soundfile as sf
from pathlib import Path

# Load mistake CSV
mistakes_path = Path("/content/outputs/examples/test_mistakes/mistakes_examples.csv")
mistakes = pd.read_csv(mistakes_path)

# Where to save audio versions
out_dir = Path("/content/outputs/examples/test_mistakes/audio")
out_dir.mkdir(parents=True, exist_ok=True)

for _, row in mistakes.iterrows():
    wav_path = row["path"]
    y, sr = librosa.load(wav_path, sr=22050, mono=True)
    name = f"{Path(wav_path).stem}__true-{row['true']}__pred-{row['pred']}.wav"
    sf.write(out_dir / name, y, sr)

print(f"Exported {len(mistakes)} misclassified audio files → {out_dir}")