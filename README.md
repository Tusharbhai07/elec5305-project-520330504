````markdown
# ðŸ”Š ELEC5305 â€” UrbanSound8K Automobile Sound Recognition  
### Hybrid CNNâ€“BiLSTM Model with Auxiliary Acoustic Features  
**Author:** Tushar Manish Khupte (SID: 520330504)  
**The University of Sydney â€” ELEC5305 (Audio & DSP)**

[![Open In Colab](https://img.shields.io/badge/Open%20in%20Colab-Notebook-orange?logo=googlecolab)](https://colab.research.google.com/drive/1tQ3kxnaScF5GZLx7VYJQ2cxjm3pbHZN9?usp=sharing)

---

## ðŸ“˜ Project Overview

This repository contains my **individual ELEC5305 final project**, implementing a **reproducible deep learning pipeline** for UrbanSound8K automobile-related sound classification.

The project focuses on **six key classes**:

- ðŸš˜ **Car Horn**  
- ðŸš¨ **Siren**  
- ðŸ”§ **Engine Idling**  
- ðŸŽµ **Street Music**  
- ðŸ¶ **Dog Bark**  
- ðŸ› ï¸ **Drilling**

### âœ” Core Features
- Log-mel spectrograms  
- Auxiliary features (ZCR, RMS, Modulation Spectrum, LPC)  
- **CNN â†’ BiLSTM hybrid architecture**  
- Early stopping using **macro-F1**  
- Dataset-fold evaluation (Train: 1â€“8, Val: 9, Test: 10)  
- Automatic export of:
  - Confusion matrix  
  - Training curves  
  - Per-class F1  
  - Audio examples (raw, denoised, mel-reconstructed)  

---

## ðŸŒ Project Website (GitHub Pages)

ðŸ‘‰ **Live Website:**  
https://tusharbhai07.github.io/elec5305-project-520330504/

This contains visuals, plots, audio examples, and a clean academic presentation of the project.

---

## ðŸš€ How to Run the Project

### **Train the model**
```bash
python ultrasound_8k_baseline_final_project.py
````

### **Regenerate plots**

```bash
python ultrasound_8k_baseline_final_project.py --plot
```

### **Export audio example triplets**

```bash
python ultrasound_8k_baseline_final_project.py --audio 2
```

---

## ðŸ“¦ Repository Structure

```
â”œâ”€â”€ docs/                       # GitHub Pages website
â”‚   â””â”€â”€ index.md
â”‚
â”œâ”€â”€ content/
â”‚   â”œâ”€â”€ UrbanSound8K/           # (Dataset not included in repo â€” user downloads)
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”‚   â”œâ”€â”€ plot_accuracy.png
â”‚   â”‚   â”œâ”€â”€ plot_loss.png
â”‚   â”‚   â”œâ”€â”€ plot_macro_f1.png
â”‚   â”‚   â”œâ”€â”€ plot_per_class_f1.png
â”‚   â”‚   â”œâ”€â”€ training_log.csv
â”‚   â”‚   â”œâ”€â”€ metrics.json
â”‚   â”‚   â””â”€â”€ examples/
â”‚   â”‚       â”œâ”€â”€ some_examples_raw_Audio_INPUT/
â”‚   â”‚       â”œâ”€â”€ some_examples_denoised_audio_OUTPUT/
â”‚   â”‚       â”œâ”€â”€ some_examples_Feature-Space_Reconstruction/
â”‚   â”‚       â””â”€â”€ audio_pairs/
â”‚   â”‚
â”‚   â””â”€â”€ us8k_cache/             # Cached MEL + aux features
â”‚
â”œâ”€â”€ ultrasound_8k_baseline_final_project.py   # Main training/evaluation script
â””â”€â”€ README.md
```

---

## ðŸ“Š Key Outputs (Included in `content/outputs/`)

* **Confusion Matrix**
* **Accuracy Curve**
* **Loss Curve**
* **Macro-F1 Curve**
* **Per-Class F1 Bar Chart**
* **Audio Example Triplets**
  (Input â†’ Wiener-denoised â†’ Mel-reconstructed)

---

## ðŸ“š Academic Notes

This project builds on and extends the **baseline CNN code** from:

> H. M. Khan, *Urban Sound Classification Using CNNs*. GitHub, 2021.
> [https://github.com/HassanMahmoodKhan/Urban-Sound-Classification-using-Convolutional-Neural-Networks](https://github.com/HassanMahmoodKhan/Urban-Sound-Classification-using-Convolutional-Neural-Networks)

All baseline components were **rewritten, expanded, and adapted** into a new reproducible pipeline with:

* Auxiliary features
* BiLSTM temporal modeling
* Feature-space audio reconstruction
* Macro-F1-driven early stopping
* Automated artifact export

---

## ðŸ™Œ Acknowledgements

* **ELEC5305 teaching team** â€” The University of Sydney
* UrbanSound8K dataset creators
* PyTorch, Librosa, Scikit-learn
* GitHub & Google Colab

---

## ðŸ“„ License

**Academic/educational use only.**
Not intended for commercial deployment.

```



